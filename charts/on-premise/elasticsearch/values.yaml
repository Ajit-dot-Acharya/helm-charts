image:
  es:
    repository: docker.elastic.co/elasticsearch/elasticsearch-oss
    tag: 6.8.6
    pullPolicy: Always
  init:
    repository: busybox
    tag: latest
    pullPolicy: IfNotPresent

# Global application tags   
# sfappname_key and sfprojectname_key should be in lowercase. 
# sfappname: petclinic --> snappyflow/projectname: elasticsearch
# sfprojectname: spring --> snappyflow/projectname: demo-kube
global:
  sfappname: escluster
  sfprojectname: scale-test
  sfappname_key: snappyflow/appname
  sfprojectname_key: snappyflow/projectname
  key: ""

common:
  # Defines the service type for all outward-facing (non-discovery) services.
  # For minikube use NodePort otherwise use LoadBalancer
  serviceType: NodePort

  env:
    CLUSTER_NAME: "myesdb"

    # Uncomment this if you get the "No up-and-running site-local (private)
    # addresses" error.
    # NETWORK_HOST: "_eth0_"

  # If enabled, then the data and master nodes will be StatefulSets with
  # associated persistent volume claims.
  stateful:
    enabled: true

    # The PVC storage class that backs the persistent volume claims. On AWS
    # "gp2" would be appropriate.
    class: "gp2"

# Client/ingest nodes can execute pre-processing pipelines, composed of
# one or more ingest processors. Depending on the type of operations performed
# by the ingest processors and the required resources, it may make sense to
# have dedicated ingest nodes, that will only perform this specific task.
client:
  # It isn't common to need more than 2 client nodes.
  replicas: 2
  antiAffinity: "soft"

  # The amount of RAM allocated to the JVM heap. This should be set to the
  # same value as client.resources.requests.memory, or you may see
  # OutOfMemoryErrors on startup.
  heapMemory: 1g

  resources:
    requests:
      cpu: 500m
      memory: 512Mi
    limits:
      cpu: 1000m
      memory: 2Gi
  env:
    NODE_DATA: "false"
    NODE_MASTER: "false"
    NODE_INGEST: "true"
    HTTP_ENABLE: "true"

# Data nodes hold the shards that contain the documents you have indexed. Data
# nodes handle data related operations like CRUD, search, and aggregations.
# These operations are I/O-, memory-, and CPU-intensive. It is important to
# monitor these resources and to add more data nodes if they are overloaded.
#
# The main benefit of having dedicated data nodes is the separation of the
# master and data roles.
data:
  # This count will depend on your data and computation needs.
  replicas: 2
  antiAffinity: "soft"

  # The amount of RAM allocated to the JVM heap. This should be set to the
  # same value as data.resources.requests.memory, or you may see
  # OutOfMemoryErrors on startup.
  heapMemory: 1g

  resources:
    requests:
      cpu: 500m
      memory: 1Gi
    limits:
      cpu: 1200m
      memory: 3500Mi
  env:
    NODE_DATA: "true"
    NODE_MASTER: "false"
    NODE_INGEST: "false"
    HTTP_ENABLE: "false"

  # Determines the properties of the persistent volume claim associated with a
  # data node StatefulSet that is created when the common.stateful.enabled
  # attribute is true.
  stateful:
    # This is a default value, and will not be sufficient in a production
    # system. You'll probably want to increase it.
    size: 60Gi

# The master node is responsible for lightweight cluster-wide actions such as
# creating or deleting an index, tracking which nodes are part of the
# cluster, and deciding which shards to allocate to which nodes. It is
# important for cluster health to have a stable master node.
master:
  # Master replica count should be (#clients / 2) + 1, and generally at least 3.
  replicas: 3
  antiAffinity: "soft"

  # The amount of RAM allocated to the JVM heap. This should be set to the
  # same value as master.resources.requests.memory, or you may see
  # OutOfMemoryErrors on startup.
  heapMemory: 1g

  resources:
    requests:
      cpu: 200m
      memory: 512Mi
    limits:
      cpu: 1000m
      memory: 2Gi

  env:
    NODE_DATA: "false"
    NODE_MASTER: "true"
    NODE_INGEST: "false"
    HTTP_ENABLE: "false"

    # The default value for this environment variable is 2, meaning a cluster
    # will need a minimum of 2 master nodes to operate. If you have 3 masters
    # and one dies, the cluster still works.
    NUMBER_OF_MASTERS: "2"

  # Determines the properties of the persistent volume claim associated with a
  # data node StatefulSet that is created when the common.stateful.enabled
  # attribute is true.
  stateful:
    # This is a default value, and will not be sufficient in a production
    # system. You'll probably want to increase it.
    size: 4Gi

service:
  httpPort: 9200
  transportPort: 9300

exporter:
  ## number of exporter instances
  ##
  replicaCount: 1

  ## restart policy for all containers
  ##
  restartPolicy: Always

  image:
    repository: justwatch/elasticsearch_exporter
    tag: 1.1.0
    pullPolicy: IfNotPresent

  resources: {}
    # requests:
    #   cpu: 100m
    #   memory: 128Mi
    # limits:
    #   cpu: 100m
    #   memory: 128Mi

  service:
    type: ClusterIP
    httpPort: 9108
    annotations: {"snappyflow/prometheus": "true"}

  es:
    ## Timeout for trying to get stats from Elasticsearch. (ex: 20s)
    ##
    timeout: 30s
  web:
    ## Path under which to expose metrics.
    ##
    path: /metrics

#image_registry: 10.11.0.217:5000
#image_registry: 34.218.206.182:5000



logger:
  enabled: false
  image: snappyflowml/sfagent
  imageTag: latest
  imagePullPolicy: Always
