global:
  hive:
    internal:
      enabled: true
    external:
      enabled: false
      # Below fields must be changed when external hive.external.enabled is true
      userName: ubuntu
      host: 127.0.0.1
      port: 10000
      auth: NONE

  postgresql:
    host: host
    postgresqlDatabase: new_archival
    postgresqlUsername: archive
    postgresqlPassword: archive123
    servicePort: 5432

  secrets:
    gcs:
      enable: false
      secret: XX
      key: XX.json
    aws:
      enable: true
      AWS_DEFAULT_REGION: us-west-2
      # Below line would be true when using IAM roles
      use_iam_role: false
      # Below is the ARN of the IAM role
      AWS_IAM_ROLE_ARN: arn:aws:iam::XXXX:role/XXXX
      # Below details are needed when we are NOT using IAM roles
      AWS_ACCESS_KEY_ID: XXXX
      AWS_SECRET_ACCESS_KEY: XXXX

  kafkaBrokers: ""

  snappyflowDatapath:
    enabled: true
    releaseName: sf-datapath

  snappyflowProjectLabel: snappyflow/projectname
  snappyflowAppLabel: snappyflow/appname

  snappyflowProjectName: "dummy-project"
  snappyflowAppName: "dummy-app"

  imagePullSecrets:
  - name: xxxx

# Pod priority class value
priority: 100

dataset-controller:
  billing-service:
    url: 127.0.0.1:8000/api/v1/records/
    scheme: https

spark-manager:
  jobserver:
    sparkProperties:
      # S3 bucket with aws prefix/folder for spark event logs e.g., for bucket location of s3://sparkhs/spark-hs configure as configured below
      logDirectory: encsfsparkhs/spark-hs

spark-history-server:
  # If spark history server is enabled "logDirectory" param of its config should be same as this
  enabled: false

  s3:
    logDirectory: encsfsparkhs/spark-hs

